{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: 1 - Tokenization and Next Token Prediction with SmolLM2\n",
    "\n",
    "**Collaborators**:\n",
    "* Roberto Rodriguez ([@Cyb3rWard0g](https://x.com/Cyb3rWard0g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmolLM2 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing SmolLM2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Tokenization (Text Without Special Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Capital of France is\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (without special tokens): ['The', 'ĠCapital', 'Ġof', 'ĠFrance', 'Ġis']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens (without special tokens):\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs (without special tokens): [504, 14937, 282, 4649, 314]\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs (without special tokens):\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text (without special tokens): The Capital of France is\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Text (without special tokens):\", tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-Level BPE Tokenization in SmolLM2\n",
    "SmolLM2 uses [byte-level Byte Pair Encoding (BPE)](https://huggingface.co/learn/nlp-course/chapter6/5), meaning spaces are included in tokens as special characters. The first token in a sequence does not include a space representation, but all subsequent tokens do. This behavior ensures proper tokenization consistency across different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Tokenization (Chat With Special Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"The Capital of France is\"}]\n",
    "non_tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "encoded_input = tokenizer(non_tokenized_chat, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  9690,   198,  2683,   359,   253,  5356,  5646, 11173,  3365,\n",
       "          3511,   308, 34519,    28,  7018,   411,   407, 19712,  8182,     2,\n",
       "           198,     1,  4093,   198,   504, 14937,   282,  4649,   314,     2,\n",
       "           198,     1,   520,  9531,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensor = encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs (with special tokens): tensor([[    1,  9690,   198,  2683,   359,   253,  5356,  5646, 11173,  3365,\n",
      "          3511,   308, 34519,    28,  7018,   411,   407, 19712,  8182,     2,\n",
      "           198,     1,  4093,   198,   504, 14937,   282,  4649,   314,     2,\n",
      "           198,     1,   520,  9531,   198]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs (with special tokens):\", input_ids_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Chat (with special tokens):\n",
      " <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "The Capital of France is<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Chat (with special tokens):\\n\", tokenizer.decode(input_ids_tensor[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Token Prediction - Autoregressive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  504, 14937,   282,  4649,   314]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input prompt\n",
    "prompt = \"The Capital of France is\"\n",
    "input_text = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SmolLM2 Efficiently\n",
    "\n",
    "To avoid downloading the model every time (**~3.42 GB**), we first check if it exists locally before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e82a163bc094c2ea8ea5325900a8b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "MODEL_DIR = \"data/smollm2\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        print(\"Loading model from local directory.\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"Downloading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "    return model\n",
    "\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Next Token Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Next Token:  Paris\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate next token logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_text)\n",
    "    logits = outputs.logits\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_id = next_token_logits.argmax()\n",
    "\n",
    "print(\"Predicted Next Token:\", tokenizer.decode(next_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Token Scores: Probabilities vs. Logits\n",
    "\n",
    "When generating text with SmolLM2, the model assigns scores to tokens based on their likelihood of being the next token. However, there are two different ways to interpret these scores:\n",
    "\n",
    "* Softmax Probabilities (Normalized Likelihoods)\n",
    "* Raw Logits (Unnormalized Scores)\n",
    "\n",
    "Both methods provide valuable insights into token selection but serve different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Probabilities: Interpreting Token Likelihood\n",
    "The first approach applies softmax normalization to convert raw logits into probabilities. These probabilities indicate how likely each token is relative to others at a given step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_top_k_predictions(input_text, model, tokenizer, k=5):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, -1, :]  # Get the logits for the last token\n",
    "    probs = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, k, dim=-1)\n",
    "    \n",
    "    for i in range(k):\n",
    "        token = tokenizer.decode(top_k_indices[0][i])\n",
    "        score = top_k_probs[0][i].item()\n",
    "        print(f\"Token: {token}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  Paris, Score: 0.8024\n",
      "Token:  the, Score: 0.0374\n",
      "Token:  a, Score: 0.0267\n",
      "Token:  known, Score: 0.0110\n",
      "Token:  called, Score: 0.0062\n"
     ]
    }
   ],
   "source": [
    "get_top_k_predictions(\"The Capital of France is\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What This Shows:\n",
    "* The probability distribution of the next possible tokens.\n",
    "* The highest probability token is the most likely next token.\n",
    "* The scores sum to 1 because of the softmax transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Logits: Interpreting Model Confidence\n",
    "The second approach examines raw logits, which are unnormalized scores produced directly by the model before softmax is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_raw_logits(input_text, model, tokenizer, k=5):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, -1, :]  # Get raw logits for the last token\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    for i in range(k):\n",
    "        token = tokenizer.decode(top_k_indices[0][i])\n",
    "        score = top_k_logits[0][i].item()\n",
    "        print(f\"Token: {token}, Logit Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  Paris, Logit Score: 18.4019\n",
      "Token:  the, Logit Score: 15.3357\n",
      "Token:  a, Logit Score: 14.9981\n",
      "Token:  known, Logit Score: 14.1096\n",
      "Token:  called, Logit Score: 13.5362\n"
     ]
    }
   ],
   "source": [
    "get_top_k_raw_logits(\"The Capital of France is\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What This Shows:\n",
    "* Raw model outputs before softmax.\n",
    "* These scores are not probabilities and can have negative values.\n",
    "* The higher the logit, the more preferred the token (but not in a probabilistic sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Probabilities vs. Logits: Which One to Use?\n",
    "\n",
    "|Approach| What It Represents | When to Use |\n",
    "| --- | --- | --- |\n",
    "|Softmax Probabilities | Normalized likelihood of a token (values between 0 and 1, sum to 1) | When you want to understand how likely each token is relative to others. |\n",
    "| Raw Logits | Unnormalized scores before softmax (can be negative, not sum to 1) | When you want to analyze model preference for tokens in absolute terms. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Token Generation\n",
    "\n",
    "The iterative decoding process mimics how SmolLM2 generates text one token at a time. At each step, the model predicts the most likely next tokens, ranks them by their raw logits, and selects the top choice. The process repeats, appending the selected token to the input until an end condition (`<|im_end|>` or EOS) is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_top_k_tokens(prompt, model, tokenizer, k=5, max_tokens=10):\n",
    "    \"\"\"\n",
    "    Generates text iteratively, displaying the top-k token predictions at each step.\n",
    "    Continues until reaching the EOS token (<|im_end|>) or max_tokens is reached.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input text to start generation.\n",
    "        model: Pretrained language model (SmolLM2).\n",
    "        tokenizer: Tokenizer corresponding to the model.\n",
    "        k (int): Number of top tokens to display at each step.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The final generated sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = []\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get raw logits for the last token\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Get top-k token indices and scores\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "\n",
    "        print(f\"\\nStep {_+1}: Top {k} Token Predictions\")\n",
    "        for i in range(k):\n",
    "            token_text = tokenizer.decode(top_k_indices[0][i])\n",
    "            logit_score = top_k_logits[0][i].item()\n",
    "            print(f\"Rank {i+1}: Token = '{token_text}', Logit Score = {logit_score:.4f}\")\n",
    "\n",
    "            # Stop if <|im_end|> appears in the top-k predictions\n",
    "            if token_text.strip() == \"<|im_end|>\":\n",
    "                print(\"\\nStopping Generation: Encountered End-of-Sequence Token (<|im_end|>) in Top-K\")\n",
    "                final_output = prompt + \"\".join(generated_tokens)\n",
    "                print(\"\\nFinal Generated Text:\", final_output)\n",
    "                return final_output\n",
    "        \n",
    "        # Select the top token\n",
    "        top_token_id = top_k_indices[0, 0].unsqueeze(0)  # Take the highest ranked token\n",
    "        top_token_text = tokenizer.decode(top_token_id)\n",
    "\n",
    "        # Append selected token to the output\n",
    "        generated_tokens.append(top_token_text)\n",
    "        \n",
    "        # Append the new token to the input sequence\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.cat([inputs[\"input_ids\"], top_token_id.unsqueeze(0)], dim=1),\n",
    "            \"attention_mask\": torch.cat([inputs[\"attention_mask\"], torch.tensor([[1]]).to(device)], dim=1),\n",
    "        }\n",
    "    \n",
    "    final_output = prompt + \"\".join(generated_tokens)\n",
    "    print(\"\\nFinal Generated Text:\", final_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Top 4 Token Predictions\n",
      "Rank 1: Token = ' Paris', Logit Score = 18.4019\n",
      "Rank 2: Token = ' the', Logit Score = 15.3357\n",
      "Rank 3: Token = ' a', Logit Score = 14.9981\n",
      "Rank 4: Token = ' known', Logit Score = 14.1096\n",
      "\n",
      "Step 2: Top 4 Token Predictions\n",
      "Rank 1: Token = '.', Logit Score = 17.5881\n",
      "Rank 2: Token = ',', Logit Score = 17.0111\n",
      "Rank 3: Token = '.\"', Logit Score = 16.2235\n",
      "Rank 4: Token = '.\",', Logit Score = 15.6103\n",
      "\n",
      "Step 3: Top 4 Token Predictions\n",
      "Rank 1: Token = '\n",
      "', Logit Score = 13.8837\n",
      "Rank 2: Token = '<|im_end|>', Logit Score = 13.6790\n",
      "\n",
      "Stopping Generation: Encountered End-of-Sequence Token (<|im_end|>) in Top-K\n",
      "\n",
      "Final Generated Text: The Capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Run the iterative token generation\n",
    "output = generate_top_k_tokens(\"The Capital of France is\", model, tokenizer, k=4, max_tokens=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
