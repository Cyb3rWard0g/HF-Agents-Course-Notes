{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: 6 - Creating a Basic ReAct Agent with SmolLM2 from Scratch\n",
    "\n",
    "**Collaborators**:\n",
    "* Roberto Rodriguez ([@Cyb3rWard0g](https://x.com/Cyb3rWard0g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ReAct Agents\n",
    "A ReAct agent follows a structured `Thought-Action-Observation` loop, allowing it to reason about its next steps, take actions, and adjust its approach based on observations. Unlike a simple ToolCallingAgent, which directly invokes tools based on a query, a ReAct agent thinks step by step before acting and refines its response through multiple iterations.\n",
    "\n",
    "In the previous notebooks, we saw how easy it is to build a `ToolCallingAgent`, where the model directly produces tool calls. Now, let's explore how we can replicate the ReAct pattern with the `SmolLM2` model, as described in [this notebook shared in the course](https://github.com/huggingface/agents-course/blob/main/notebooks/unit1/dummy_agent_library.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class LMClient:\n",
    "    \"\"\"\n",
    "    Handles communication with SmolLM2 for generating responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, max_new_tokens=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def generate(self, messages: List[Dict], stop_sequences: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response from SmolLM2 given a conversation history, stopping at 'Observation:' if provided.\n",
    "\n",
    "        Args:\n",
    "            messages (List[Dict]): The list of messages in the chat.\n",
    "            stop_sequences (List[str], optional): Sequences that signal generation should stop.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        encoded_input = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "        stopping_criteria = None\n",
    "        if stop_sequences:\n",
    "            stopping_criteria = self.make_stopping_criteria(stop_sequences)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        generated_tokens = outputs[0][input_ids.shape[1]:]\n",
    "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    def make_stopping_criteria(self, stop_sequences: List[str]) -> StoppingCriteriaList:\n",
    "        \"\"\"\n",
    "        Creates a stopping criterion that halts generation when any of the stop sequences are reached.\n",
    "\n",
    "        Args:\n",
    "            stop_sequences (List[str]): A list of stop sequences.\n",
    "\n",
    "        Returns:\n",
    "            StoppingCriteriaList: Custom stopping criteria.\n",
    "        \"\"\"\n",
    "\n",
    "        class StopOnStrings(StoppingCriteria):\n",
    "            def __init__(self, stop_strings, tokenizer):\n",
    "                self.stop_strings = stop_strings\n",
    "                self.tokenizer = tokenizer\n",
    "                self.generated_text = \"\"  # Store generated text stream\n",
    "\n",
    "            def __call__(self, input_ids, scores, **kwargs):\n",
    "                \"\"\"Stop generation when any stop sequence is found in the accumulated output.\"\"\"\n",
    "                self.generated_text += self.tokenizer.decode(input_ids[0][-1], skip_special_tokens=True)\n",
    "\n",
    "                # Stop if any stop sequence appears in the generated text\n",
    "                return any(stop_seq in self.generated_text for stop_seq in self.stop_strings)\n",
    "\n",
    "        return StoppingCriteriaList([StopOnStrings(stop_sequences, self.tokenizer)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tool Class and Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Represents an AI-registered tool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, description: str, func: callable):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = inspect.signature(func).parameters\n",
    "        self.outputs = inspect.signature(func).return_annotation\n",
    "    \n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a structured representation of the tool.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([f\"{arg}: {param.annotation}\" for arg, param in self.arguments.items()])\n",
    "        return f\"Tool Name: {self.name}, Description: {self.description}, Arguments: {args_str}, Outputs: {self.outputs}\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Invoke the tool.\"\"\"\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "def tool(func):\n",
    "    \"\"\"\n",
    "    Decorator to register a function as a tool.\n",
    "    \"\"\"\n",
    "    return Tool(func.__name__, func.__doc__, func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "SYSTEM_PROMPT = Template(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{{ tools }}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "### ReAct Thought-Action-Observation Cycle\n",
    "\n",
    "1. **Thought**: First, analyze the query and determine what action to take.\n",
    "2. **Action**: Call a tool if needed, using a structured JSON format.\n",
    "3. **Observation**: Wait for the tool's response before making the next decision.\n",
    "\n",
    "#### Example Format:\n",
    "Question: the input question you must answer\n",
    "Thought: reason about what to do next\n",
    "Action:\n",
    "\n",
    "{\n",
    "  \"action\": \"tool_name\",\n",
    "  \"action_input\": { \"param1\": \"value1\" }\n",
    "}\n",
    "\n",
    "Observation: result from the tool execution\n",
    "... (this Thought/Action/Observation sequence can repeat N times)\n",
    "\n",
    "You must always end your output with:\n",
    "\n",
    "Thought: I now know the final answer\n",
    "Final Answer: <your answer here>\n",
    "\n",
    "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tool Calling Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    Implements a ReAct-style agent that follows the Thought-Action-Observation cycle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: LMClient):\n",
    "        self.model = model\n",
    "        self.system_prompt = SYSTEM_PROMPT\n",
    "        self.tools = {}\n",
    "\n",
    "    def register_tool(self, tool: Tool):\n",
    "        \"\"\"\n",
    "        Registers a tool for function calling.\n",
    "\n",
    "        Args:\n",
    "            tool (Tool): The tool instance.\n",
    "        \"\"\"\n",
    "        if not isinstance(tool, Tool):\n",
    "            raise TypeError(f\"Expected Tool instance, got {type(tool)}\")\n",
    "        self.tools[tool.name] = tool\n",
    "        logger.info(f\"Registered tool: {tool.name}\")\n",
    "\n",
    "    def prepare_messages(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Prepares structured messages including system instructions.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, str]]: Formatted conversation messages.\n",
    "        \"\"\"\n",
    "        tool_descriptions = \"\\n\".join([t.to_string() for t in self.tools.values()])\n",
    "        rendered_prompt = self.system_prompt.render(tools=tool_descriptions)\n",
    "\n",
    "        system_message = {\"role\": \"system\", \"content\": rendered_prompt}\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"Question: {query}\\n\"}  # Start accumulating\n",
    "\n",
    "        return [system_message, user_message]\n",
    "\n",
    "    def parse_response(self, text: str) -> Tuple[Optional[str], Optional[dict], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Extracts the thought, action, and final answer (if present) from the model response.\n",
    "\n",
    "        Args:\n",
    "            text (str): The model-generated response.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: (thought content, action dictionary if present, final answer if present)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Received response from model: {text}\")\n",
    "\n",
    "        # Define regex patterns\n",
    "        action_regex = re.compile(r'Action:\\s*({.*?})', re.DOTALL)  # Extract JSON action\n",
    "        final_answer_regex = re.compile(r'Final Answer:\\s*(.*)', re.DOTALL)  # Extract final answer\n",
    "        thought_regex = re.compile(r'Thought:\\s*(.*?)$', re.DOTALL | re.MULTILINE)  # Extract thought\n",
    "\n",
    "        thought, action, final_answer = None, None, None\n",
    "\n",
    "        # Extract thought\n",
    "        thought_match = thought_regex.search(text)\n",
    "        if thought_match:\n",
    "            thought = thought_match.group(1).strip()\n",
    "\n",
    "        # Extract action JSON\n",
    "        action_match = action_regex.search(text)\n",
    "        if action_match:\n",
    "            try:\n",
    "                action = json.loads(action_match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"Invalid action JSON: {action_match.group(1)}\")\n",
    "                raise ValueError(f\"Failed to parse action JSON: {action_match.group(1)}\")\n",
    "\n",
    "        # Extract final answer\n",
    "        final_answer_match = final_answer_regex.search(text)\n",
    "        if final_answer_match:\n",
    "            final_answer = final_answer_match.group(1).strip()\n",
    "\n",
    "        return thought, action, final_answer\n",
    "\n",
    "    def _execute_tool_calls(self, action: dict) -> str:\n",
    "        \"\"\"\n",
    "        Executes the requested tool function and returns the observation.\n",
    "\n",
    "        Args:\n",
    "            action (dict): The tool call JSON.\n",
    "\n",
    "        Returns:\n",
    "            str: Observation string containing the tool execution result.\n",
    "        \"\"\"\n",
    "        tool_name = action.get(\"action\")\n",
    "        tool_args = action.get(\"action_input\", {})\n",
    "\n",
    "        if tool_name not in self.tools:\n",
    "            observation = f\"Error: Unknown tool {tool_name}\"\n",
    "            logger.error(observation)\n",
    "            return observation\n",
    "\n",
    "        logger.info(f\"Executing tool: {tool_name} with arguments {tool_args}\")\n",
    "        tool_result = self.tools[tool_name](**tool_args)\n",
    "\n",
    "        return f\"Observation: {tool_result}\\n\"\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Processes a user query using the Thought-Action-Observation cycle.\n",
    "\n",
    "        Args:\n",
    "            query (str): User query.\n",
    "\n",
    "        Returns:\n",
    "            str: The final natural response from the assistant.\n",
    "        \"\"\"\n",
    "        logger.info(f\"User query received: {query}\")\n",
    "        messages = self.prepare_messages(query)\n",
    "\n",
    "        while True:  # Loop until \"Final Answer\" is found\n",
    "            response_text = self.model.generate(messages, stop_sequences=[\"Observation:\"])\n",
    "\n",
    "            thought, action, final_answer = self.parse_response(response_text)\n",
    "\n",
    "            # Log current cycle\n",
    "            logger.info(f\"Thought: {thought}\")\n",
    "            logger.info(f\"Action: {action}\")\n",
    "\n",
    "            if final_answer:\n",
    "                logger.info(f\"Final response from assistant: {final_answer}\")\n",
    "                return final_answer\n",
    "\n",
    "            if action:\n",
    "                observation = self._execute_tool_calls(action)\n",
    "\n",
    "                # Update user message content instead of adding new messages\n",
    "                messages[1][\"content\"] += f\"Thought: {thought}\\n\"\n",
    "                messages[1][\"content\"] += f\"Action:\\n{json.dumps(action, indent=2)}\\n\"\n",
    "                messages[1][\"content\"] += f\"{observation}\\n\"\n",
    "\n",
    "                logger.info(f\"Continuing cycle with observation: {observation}\")\n",
    "\n",
    "            else:\n",
    "                logger.warning(\"No action detected, breaking loop.\")\n",
    "                return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing SmolLM2 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SmolLM2 Efficiently\n",
    "\n",
    "To avoid downloading the model every time (**~3.42 GB**), we first check if it exists locally before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4da4255e4f448fa40634e42156d4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "MODEL_DIR = \"data/smollm2\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        print(\"Loading model from local directory.\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"Downloading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Language Model Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMClient(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools\n",
    "\n",
    "Tools allow the model to execute external functions when needed. We define them as Python functions and convert them into structured format for SmolLM2 to understand their purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Returns the current time in HH:MM:SS format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "@tool\n",
    "def get_random_number(min: int, max: int) -> int:\n",
    "    \"\"\"Returns a random number between min and max.\"\"\"\n",
    "    return random.randint(min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool Name: get_current_time, Description: Returns the current time in HH:MM:SS format., Arguments: , Outputs: <class 'str'>\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool Name: get_random_number, Description: Returns a random number between min and max., Arguments: min: <class 'int'>, max: <class 'int'>, Outputs: <class 'int'>\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_number.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:37:14,561 - INFO - Registered tool: get_current_time\n",
      "2025-02-17 20:37:14,561 - INFO - Registered tool: get_random_number\n"
     ]
    }
   ],
   "source": [
    "agent.register_tool(get_current_time)\n",
    "agent.register_tool(get_random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get_current_time': <__main__.Tool at 0x754a75d10>,\n",
       " 'get_random_number': <__main__.Tool at 0x14fcaf820>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic One-Step Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:37:15,460 - INFO - User query received: What is the current time?\n",
      "2025-02-17 20:37:30,306 - INFO - Received response from model: Thought: I will use the get_current_time tool to get the current time.\n",
      "\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"get_current_time\"\n",
      "}\n",
      "\n",
      "Observation:\n",
      "2025-02-17 20:37:30,309 - INFO - Thought: I will use the get_current_time tool to get the current time.\n",
      "2025-02-17 20:37:30,309 - INFO - Action: {'action': 'get_current_time'}\n",
      "2025-02-17 20:37:30,310 - INFO - Executing tool: get_current_time with arguments {}\n",
      "2025-02-17 20:37:30,312 - INFO - Continuing cycle with observation: Observation: 20:37:30\n",
      "\n",
      "2025-02-17 20:37:43,551 - INFO - Received response from model: Thought: I now know the current time is 20:37:30.\n",
      "Final Answer: 20:37:30\n",
      "2025-02-17 20:37:43,552 - INFO - Thought: I now know the current time is 20:37:30.\n",
      "2025-02-17 20:37:43,554 - INFO - Action: None\n",
      "2025-02-17 20:37:43,554 - INFO - Final response from assistant: 20:37:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20:37:30'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"What is the current time?\")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:38:29,564 - INFO - User query received: Give me a random number between 1 and 10.\n",
      "2025-02-17 20:38:50,494 - INFO - Received response from model: {\n",
      "  \"action\": \"get_random_number\",\n",
      "  \"action_input\": {\n",
      "    \"min\": 1,\n",
      "    \"max\": 10\n",
      "  }\n",
      "}\n",
      "\n",
      "Thought: I now know the final answer: 4\n",
      "Final Answer: 4\n",
      "2025-02-17 20:38:50,504 - INFO - Thought: I now know the final answer: 4\n",
      "2025-02-17 20:38:50,505 - INFO - Action: None\n",
      "2025-02-17 20:38:50,505 - INFO - Final response from assistant: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"Give me a random number between 1 and 10.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
