{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: 2 - Beam Search with GPT2\n",
    "\n",
    "**Collaborators**:\n",
    "* Roberto Rodriguez ([@Cyb3rWard0g](https://x.com/Cyb3rWard0g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GPT2 Efficiently\n",
    "\n",
    "To avoid downloading the model every time (**~548 MB**), we first check if it exists locally before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142880e6b3564edfb19e3ab88b4ed79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c4c73ac3dd41bdab669e2f18fd69e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9bcca5564d4b3994c6e48ad9d71b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78407f77e5f43f7b590b3c47a8f10b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd8fbc05c054f3a85b7ff15aa1723f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4851adffcdba4e3c8f3fb510c57f4e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c26664fea8b469d8ac469115a9ade43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"openai-community/gpt2\"\n",
    "MODEL_DIR = \"data/gpt2\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        print(\"Loading model from local directory.\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"Downloading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Token Prediction: The Core of LLMs\n",
    "\n",
    "Large Language Models (LLMs) are **autoregressive**, meaning they generate text **one token at a time** based on previous context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Next Token Prediction Works:**\n",
    "- The model takes an input sequence.\n",
    "- It generates a probability distribution over possible next tokens.\n",
    "- The most likely token (or one sampled using randomness) is selected.\n",
    "- The process repeats until an **End of Sequence (EOS)** token is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Next Token:  the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "# Generate next token logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    next_token_id = logits.argmax()\n",
    "\n",
    "print(\"Predicted Next Token:\", tokenizer.decode(next_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search: Improving Generated Sequences\n",
    "\n",
    "While greedy decoding picks the single highest probability token at each step, **beam search** keeps multiple \"beams\" (hypotheses) and explores various possibilities before finalizing an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Beam Search Works:\n",
    "- Maintains multiple possible token sequences (**num_beams**).\n",
    "- Expands each sequence by considering multiple likely next tokens.\n",
    "- Applies a **length penalty** to avoid bias toward shorter sequences.\n",
    "- Returns the top **num_return_sequences** outputs based on cumulative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(\n",
    "    prompt, model, tokenizer, \n",
    "    max_new_tokens=12, num_beams=4, length_penalty=1.0, num_return_sequences=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text using Beam Search Decoding.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text to decode.\n",
    "    - model: The SmolLM2 model.\n",
    "    - tokenizer: The tokenizer for SmolLM2.\n",
    "    - max_new_tokens (int): Number of tokens to generate (steps).\n",
    "    - num_beams (int): Number of beams to use.\n",
    "    - length_penalty (float): Length penalty (higher promotes longer sequences).\n",
    "    - num_return_sequences (int): Number of top-ranked sequences to return.\n",
    "\n",
    "    Returns:\n",
    "    - List of generated sequences ranked by score.\n",
    "    \"\"\"\n",
    "    # Encode raw text input with attention mask\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = encoded_input[\"input_ids\"]\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=length_penalty,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_sequences = [tokenizer.decode(output, skip_special_tokens=False) for output in outputs]\n",
    "\n",
    "    print(\"\\n=== Beam Search Results ===\\n\")\n",
    "    for i, seq in enumerate(generated_sequences):\n",
    "        print(f\"Rank {i+1}: {seq}\")\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beam Search Results ===\n",
      "\n",
      "Rank 1: Conclusion: thanks a lot. That's all for today.\n",
      "\n",
      "Advertisements<|endoftext|>\n",
      "Rank 2: Conclusion: thanks a lot. That's all for today. I hope you enjoyed\n",
      "Rank 3: Conclusion: thanks a lot. That's all for today. I'll be back\n"
     ]
    }
   ],
   "source": [
    "# Define input sentence\n",
    "prompt = \"Conclusion: thanks a lot. That's all for today\"\n",
    "\n",
    "# Run beam search decoding\n",
    "beam_search_results = generate_with_beam_search(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    max_new_tokens=5,        # Number of steps\n",
    "    num_beams=4,             # Number of beams\n",
    "    length_penalty=1.0,      # Length penalty\n",
    "    num_return_sequences=3   # Number of return sequences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
