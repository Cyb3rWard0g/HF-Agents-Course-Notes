{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: 5 - Creating a Tool Calling Agent with SmolLM2 from Scratch\n",
    "\n",
    "**Collaborators**:\n",
    "* Roberto Rodriguez ([@Cyb3rWard0g](https://x.com/Cyb3rWard0g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build an autonomous **Tool-Calling Agent** using SmolLM2. The agent intelligently determines when a function call is needed and executes the corresponding tool when required. We explore how SmolLM2 structures tool calls and integrate a flexible system for executing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class LMClient:\n",
    "    \"\"\"\n",
    "    Handles communication with SmolLM2 for generating responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, max_new_tokens = 512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def generate(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response from SmolLM2 given a conversation history.\n",
    "\n",
    "        Args:\n",
    "            messages (List[Dict]): The list of messages in the chat.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        # Convert messages into model-compatible format\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Encode input with attention mask\n",
    "        encoded_input = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode assistant response\n",
    "        generated_tokens = outputs[0][input_ids.shape[1]:]\n",
    "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tool Class and Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Represents an AI-registered tool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, description: str, func: callable):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = inspect.signature(func).parameters\n",
    "        self.outputs = inspect.signature(func).return_annotation\n",
    "    \n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a structured representation of the tool.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([f\"{arg}: {param.annotation}\" for arg, param in self.arguments.items()])\n",
    "        return f\"Tool Name: {self.name}, Description: {self.description}, Arguments: {args_str}, Outputs: {self.outputs}\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Invoke the tool.\"\"\"\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def tool(func):\n",
    "    \"\"\"\n",
    "    Decorator to register a function as a tool.\n",
    "    \"\"\"\n",
    "    return Tool(func.__name__, func.__doc__, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define System Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "SYSTEM_PROMPT = Template(\"\"\"\n",
    "You are an AI assistant that can use tools when needed. \n",
    "You will decide when to call a tool based on the user's query. \n",
    "\n",
    "If a tool is needed, return output in this format:\n",
    "<tool_call>[\n",
    "{\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\n",
    "... (more tool calls as required)\n",
    "]</tool_call>\n",
    "\n",
    "Otherwise, respond naturally.\n",
    "You have access to the following tools:\n",
    "<tools>{{ tools }}</tools>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tool Calling Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "class ToolCallingAgent:\n",
    "    \"\"\"\n",
    "    Implements a tool-calling agent for SmolLM2, integrating tool execution\n",
    "    and structured response parsing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: LMClient):\n",
    "        self.model = model\n",
    "        self.system_prompt = SYSTEM_PROMPT\n",
    "        self.tools = {}\n",
    "        self.history = []\n",
    "\n",
    "    def register_tool(self, tool: Tool):\n",
    "        \"\"\"\n",
    "        Registers a tool for function calling.\n",
    "\n",
    "        Args:\n",
    "            tool (Tool): The tool instance.\n",
    "        \"\"\"\n",
    "        if not isinstance(tool, Tool):\n",
    "            raise TypeError(f\"Expected Tool instance, got {type(tool)}\")\n",
    "        self.tools[tool.name] = tool\n",
    "        logger.info(f\"Registered tool: {tool.name}\")\n",
    "\n",
    "    def prepare_messages(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Prepares structured messages including system instructions.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, str]]: Formatted conversation messages.\n",
    "        \"\"\"\n",
    "        tool_descriptions = \"\\n\".join([t.to_string() for t in self.tools.values()])\n",
    "        rendered_prompt = self.system_prompt.render(tools=tool_descriptions)\n",
    "        \n",
    "        system_message = {\"role\": \"system\", \"content\": rendered_prompt}\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        messages = [system_message] + self.history\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def parse_response(self, text: str) -> Any:\n",
    "        \"\"\"\n",
    "        Parses SmolLM2 response, extracting tool calls if present.\n",
    "\n",
    "        Args:\n",
    "            text (str): The model-generated response.\n",
    "\n",
    "        Returns:\n",
    "            Any: Parsed tool calls or direct assistant response.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Received response from model: {text}\")\n",
    "\n",
    "        pattern = r\"<tool_call>(.*?)</tool_call>\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            tool_calls = json.loads(match.group(1))\n",
    "            logger.info(f\"Extracted tool calls: {tool_calls}\")\n",
    "            return tool_calls\n",
    "        \n",
    "        logger.info(\"No tool calls detected, returning direct response.\")\n",
    "        return text\n",
    "\n",
    "    def _execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Executes the requested tool functions and formats assistant-user message pairs.\n",
    "\n",
    "        Args:\n",
    "            tool_calls (List[Dict[str, Any]]): List of tool calls.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, str]]: Formatted assistant-user message pairs.\n",
    "        \"\"\"\n",
    "        tool_history = []\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_call_id = \"\".join(random.choices(\"0123456789\", k=5))\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"arguments\"]\n",
    "\n",
    "            logger.info(f\"Executing tool: {tool_name} with arguments {tool_args}\")\n",
    "\n",
    "            if tool_name in self.tools:\n",
    "                tool_result = self.tools[tool_name](**tool_args)\n",
    "            else:\n",
    "                tool_result = f\"Error: Unknown tool {tool_name}\"\n",
    "                logger.error(tool_result)\n",
    "\n",
    "            # Assistant tool call message\n",
    "            assistant_tool_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Tool Chosen (id: {tool_call_id}) -> {tool_name}\"\n",
    "            }\n",
    "\n",
    "            # User tool response message\n",
    "            user_tool_response = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Tool Execution Results (id: {tool_call_id}) -> {json.dumps(tool_result)}\"\n",
    "            }\n",
    "\n",
    "            tool_history.append(assistant_tool_message)\n",
    "            tool_history.append(user_tool_response)\n",
    "\n",
    "        return tool_history\n",
    "\n",
    "    def run(self, query: str) -> Any:\n",
    "        \"\"\"\n",
    "        Processes a user query, generates a response, and executes tools if needed.\n",
    "\n",
    "        Args:\n",
    "            query (str): User query.\n",
    "\n",
    "        Returns:\n",
    "            Any: The final natural response from the assistant.\n",
    "        \"\"\"\n",
    "        logger.info(f\"User query received: {query}\")\n",
    "        messages = self.prepare_messages(query)\n",
    "\n",
    "        # Generate response\n",
    "        response_text = self.model.generate(messages)\n",
    "\n",
    "        # Parse response\n",
    "        parsed_response = self.parse_response(response_text)\n",
    "\n",
    "        # If tool calls were made, execute them\n",
    "        if isinstance(parsed_response, list):\n",
    "            tool_execution_messages = self._execute_tool_calls(parsed_response)\n",
    "\n",
    "            # Add tool execution messages to conversation\n",
    "            messages.extend(tool_execution_messages)\n",
    "\n",
    "            logger.info(f\"Tool execution completed. Sending tool results back to model for final response.\")\n",
    "\n",
    "            # Ask model to summarize the final response based on the query and tool results\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Based on the original question: '{query}', and the tool execution results, provide a clear and natural response.\"\n",
    "            })\n",
    "\n",
    "            # Generate final assistant response\n",
    "            final_response_text = self.model.generate(messages)\n",
    "\n",
    "            # Store only the final response in history\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": final_response_text})\n",
    "\n",
    "            logger.info(f\"Final response from assistant: {final_response_text}\")\n",
    "            return final_response_text\n",
    "\n",
    "        # Otherwise, return natural response\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": parsed_response})\n",
    "        logger.info(f\"Returning direct response: {parsed_response}\")\n",
    "        return parsed_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing SmolLM2 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SmolLM2 Efficiently\n",
    "\n",
    "To avoid downloading the model every time (**~3.42 GB**), we first check if it exists locally before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa42cb5c714c47eaad682ac5447c969c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "MODEL_DIR = \"data/smollm2\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        print(\"Loading model from local directory.\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"Downloading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Language Model Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMClient(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools\n",
    "\n",
    "Tools allow the model to execute external functions when needed. We define them as Python functions and convert them into JSON schemas for SmolLM2 to understand their purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Returns the current time in HH:MM:SS format.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "@tool\n",
    "def get_random_number(min: int, max: int) -> int:\n",
    "    \"\"\"Returns a random number between min and max.\"\"\"\n",
    "    return random.randint(min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool Name: get_current_time, Description: Returns the current time in HH:MM:SS format., Arguments: , Outputs: <class 'str'>\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool Name: get_random_number, Description: Returns a random number between min and max., Arguments: min: <class 'int'>, max: <class 'int'>, Outputs: <class 'int'>\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_number.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ToolCallingAgent(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:47:00,521 - INFO - Registered tool: get_current_time\n",
      "2025-02-17 17:47:00,522 - INFO - Registered tool: get_random_number\n"
     ]
    }
   ],
   "source": [
    "agent.register_tool(get_current_time)\n",
    "agent.register_tool(get_random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get_current_time': <__main__.Tool at 0x146aebcb0>,\n",
       " 'get_random_number': <__main__.Tool at 0x146aeb690>}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic One-Step Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Memory\n",
    "agent.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:47:02,200 - INFO - User query received: What is the current time?\n",
      "2025-02-17 17:47:04,746 - INFO - Received response from model: <tool_call>[{\"name\": \"get_current_time\", \"arguments\": {}}]</tool_call>\n",
      "2025-02-17 17:47:04,746 - INFO - Extracted tool calls: [{'name': 'get_current_time', 'arguments': {}}]\n",
      "2025-02-17 17:47:04,746 - INFO - Executing tool: get_current_time with arguments {}\n",
      "2025-02-17 17:47:04,746 - INFO - Tool execution completed. Sending tool results back to model for final response.\n",
      "2025-02-17 17:47:05,952 - INFO - Final response from assistant: The current time is 17:47:04.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current time is 17:47:04.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"What is the current time?\")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the current time?'},\n",
       " {'role': 'assistant', 'content': 'The current time is 17:47:04.'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:47:08,796 - INFO - User query received: Give me a random number between 1 and 10.\n",
      "2025-02-17 17:47:09,948 - INFO - Received response from model: The random number between 1 and 10 is 7.\n",
      "2025-02-17 17:47:09,949 - INFO - No tool calls detected, returning direct response.\n",
      "2025-02-17 17:47:09,949 - INFO - Returning direct response: The random number between 1 and 10 is 7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The random number between 1 and 10 is 7.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"Give me a random number between 1 and 10.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:47:11,282 - INFO - User query received: What is the capital of France?\n",
      "2025-02-17 17:47:12,029 - INFO - Received response from model: The capital of France is Paris.\n",
      "2025-02-17 17:47:12,029 - INFO - No tool calls detected, returning direct response.\n",
      "2025-02-17 17:47:12,030 - INFO - Returning direct response: The capital of France is Paris.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"What is the capital of France?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Multi-Step Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:47:13,567 - INFO - User query received: Tell me the current time and give me a random number from 10 to 20\n",
      "2025-02-17 17:47:17,122 - INFO - Received response from model: <tool_call>[{\"name\": \"get_current_time\", \"arguments\": {}}, {\"name\": \"get_random_number\", \"arguments\": {\"min\": 10, \"max\": 20}}]</tool_call>\n",
      "2025-02-17 17:47:17,122 - INFO - Extracted tool calls: [{'name': 'get_current_time', 'arguments': {}}, {'name': 'get_random_number', 'arguments': {'min': 10, 'max': 20}}]\n",
      "2025-02-17 17:47:17,123 - INFO - Executing tool: get_current_time with arguments {}\n",
      "2025-02-17 17:47:17,123 - INFO - Executing tool: get_random_number with arguments {'min': 10, 'max': 20}\n",
      "2025-02-17 17:47:17,123 - INFO - Tool execution completed. Sending tool results back to model for final response.\n",
      "2025-02-17 17:47:20,682 - INFO - Final response from assistant: Based on the original question and the tool execution results, I can provide a clear and natural response.\n",
      "\n",
      "The current time is \"17:47:17\". A random number between 10 and 20 is 19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the original question and the tool execution results, I can provide a clear and natural response.\\n\\nThe current time is \"17:47:17\". A random number between 10 and 20 is 19.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset Memoty\n",
    "agent.history = []\n",
    "\n",
    "# New question\n",
    "response = agent.run(\"Tell me the current time and give me a random number from 10 to 20\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
