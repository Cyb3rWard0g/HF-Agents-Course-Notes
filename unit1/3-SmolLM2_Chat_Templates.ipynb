{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: 3 - Chat Templates SmolLM2\n",
    "\n",
    "**Collaborators**:\n",
    "* Roberto Rodriguez ([@Cyb3rWard0g](https://x.com/Cyb3rWard0g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmolLM2 Chat Templates\n",
    "\n",
    "[SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) uses structured chat templates to format user-assistant interactions. Unlike single-turn prompts, chat-based models require proper formatting to maintain conversation history. These templates ensure that every model, despite having unique special tokens, receives properly structured input.\n",
    "\n",
    "To explore chat templates, we first need to load the SmolLM2 model and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SmolLM2 Efficiently\n",
    "\n",
    "To avoid downloading the model every time (**~3.42 GB**), we first check if it exists locally before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517ea8ea22d846daa3509874d681e92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "MODEL_DIR = \"data/smollm2\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        print(\"Loading model from local directory.\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"Downloading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model.save_pretrained(MODEL_DIR)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Chat Templates\n",
    "Chat templates are responsible for formatting conversational exchanges before they are passed to the model. This ensures the model can differentiate between system instructions, user messages, and assistant responses.\n",
    "\n",
    "### Inspecting SmolLM2 Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Template Format:\n",
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# View the chat template format used by SmolLM2\n",
    "template = tokenizer.chat_template\n",
    "print(\"Chat Template Format:\")\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Chat Template in a Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant focused on technical topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain what a chat template is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A chat template structures conversations between users and AI models...\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I use it?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant focused on technical topics.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you explain what a chat template is?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A chat template structures conversations between users and AI models...<|im_end|>\n",
      "<|im_start|>user\n",
      "How do I use it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert messages into a properly formatted prompt\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the Formatted Prompt to the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode raw text input with attention mask\n",
    "encoded_input = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "count_prompt_tokens = input_ids.shape[1]  # Save prompt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: To use a chat template, you would typically input a message into the chat interface, and the template would generate a response based on the predefined structure. The template would usually include placeholders for user input, such as \"What's your name?\" or\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, # Avoids padding/EOS confusion\n",
    "    max_new_tokens=50, \n",
    "    eos_token_id=tokenizer.eos_token_id  # Ensures stopping when EOS is reached\n",
    ")\n",
    "\n",
    "# Extract only assistant-generated tokens\n",
    "generated_tokens = outputs[0, count_prompt_tokens:]\n",
    "\n",
    "# Decode assistant response\n",
    "output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(\"Assistant Response:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models vs. Instruct Models\n",
    "- **Base Models**: Trained on raw text data and predict the next token.\n",
    "- **Instruct Models**: Fine-tuned to follow structured prompts (chat templates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Chat templates are essential for structuring conversations.\n",
    "- They ensure SmolLM2 understands multi-turn interactions properly.\n",
    "- Using `apply_chat_template()` simplifies formatting user-agent exchanges.\n",
    "- Base models require manual structuring, whereas instruct models automatically handle formatted conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
